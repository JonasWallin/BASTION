---
title: "MCMC"
output: rmarkdown::html_vignette
author: "Ray, Isaac"
vignette: >
  %\VignetteIndexEntry{MCMC}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup, include = FALSE}
library(BASTION)
library(ggplot2)
library(ggraph)
library(gridExtra)
library(igraph)
library(microbenchmark)
library(Rfast)
library(xtable)
library(fields)
library(Matrix)
library(FNN)
library(mgcv)
library(fdaPDE)
library(gamair)
```

The Bayesian Additive Regression Spanning Trees model (BAST) is a novel ensemble model for non-parametric regression intended to be used on data (especially spatial) that lies on a complex or constrained domain, or a space with irregular shape embedded in Euclidean space. At the core of the BAST model is a novel weak learner; a random spanning tree manifold partition model. The model is based upon 4 possible moves/graph operations: birth, death, change, and hyper. The `BASTION` package implements those 4 core graph operations as well as some utility functions to make it easier to utilize them.

The purpose of this vignette is to:

  1. Provide a motivating example problem using synthetic data
  2. Demonstrate how the core functions of the `BASTION` package work individually on the data
  3. Give a simple example of how the core functions can be combined to predict responses at testing data

# The Data & Motivation

We'll start by posing a situation. Suppose we are observing spatial points around a black hole, but have no data from within the black hole (our surveyors got too close). Below is a plot of all the locations at which we gathered data (stored in `data_hole`):

```{r data-gen, echo = FALSE}
set.seed(12345)
x = runif(1000, min = -10, max = 10)
y = runif(1000, min = -10, max = 10)
data_full = data.frame(x, y)
data_hole = data_full[x^2 + y^2 >= 16, ]
```

```{r data-plot, fig.width = 6, fig.height = 6}
ggplot(data_hole) + geom_point(aes(x, y))
```

At each data point, our space surveyors observed the value of the function we are interested in (stored in `response`). Below is a plot of the response data we observed at each point:


```{r data-response, echo = FALSE}
response = 1.1^(data_hole$x) + 
  (data_hole$y)^2 - 
  350*(data_hole$x > data_hole$y) + 
  200*(-data_hole$x > data_hole$y) +
  5*data_hole$x*data_hole$y

```

```{r response-plot, fig.width = 6, fig.height = 6}
ggplot(data_hole) + 
  geom_point(aes(x, y, color = response)) +
  scale_color_viridis()
```

Unfortunately, all of our space surveyors have been sucked into the black hole. But we still need a way to predict the value of our function of interest at new data points. Before discussing how we will achieve this, let's discuss the core functions of `BASTION`.

# The constructGraph function

Before we can do any graph based procedures on our data, we need to have a graph! The `BASTION` package provides the function `constructGraph` to do this for us. `constructGraph` takes in two dimensional numeric data (in our case, `data_hole`) and a parameter `k` and performs the K-Nearest Neighbors algorithm to generate a weighted `igraph` graph. Below is a plot of what our graph looks like:

```{r make-graph, fig.width = 6, fig.height = 6}
hole_graph = constructGraph(data_hole, 5)
ggraph(hole_graph, layout = data_hole) + 
  geom_edge_link0(edge_alpha = 0.1) + 
  geom_node_point(aes(color = response)) +
  scale_color_viridis()
```

An important consideration is that the K-Nearest Neighbors algorithm is not guaranteed to generate a connected graph, especially in situations where there is not much data and the chosen `k` parameter is small. Other methods of constructing a graph, such as Constrained Delaunay triangulation, may be a better choice in such instances. The other functions of the `BASTION` package do not require that the original graph is made using the `constructGraph` function, a weighted `igraph` graph generated by other means will still work so long as it is connected and points share an edge with other points that are 'nearby' (we will be using Euclidean distance for simplicity).

# Initializing Clusters

Now that we have a graph, let's separate it into clusters. We are working on the principle that points which are 'close' together in space likely also have response values which are 'close' together. However, as we will see later, this assumption need not be true once we start performing different graph operations. With this in mind, the `constructClusters` function will form a partition of our graph into `nclust` different clusters (10 is chosen arbitrarily)  without needing to consider our `response` data.

The way that `constructClusters` creates the partition is by first using Prim's algorithm to find a minimum spanning tree on the input graph `hole_graph` using its edge weights (which was generated by `constructGraph`). By property of spanning trees, by removing `nclust - 1` edges from this spanning tree, we are left with `nclust` disconnected trees which together form a spanning forest. We then assign all the vertices in the same disconnected tree to the same cluster. The final spanning forest and cluster membership vector are both available in the list output by `constructClusters`.

Below is a demonstration of `constructClusters` on our `hole_graph` and a plot of which vertices were assigned to each cluster.

```{r make-cluster, fig.width = 6, fig.height = 6}
clustered_output = constructClusters(hole_graph, nclust = 10)
ggraph(clustered_output$spanning_forest, layout = data_hole) +
  geom_edge_link0(edge_alpha = 0.1) +
  geom_node_point(aes(colour = as.factor(clustered_output$membership))) +
  scale_colour_brewer(palette = "Paired") + 
  labs(colour = "Cluster")
```

# Graph Operations

Now that we have a graph with clusters, we can demonstrate each of the four graph operations which are the core of the `BASTION` package.

## Birth Move

First, we will consider a cluster birth move. The `graphBirth` function, when provided with a clustered spanning forest graph and corresponding membership (`spanning_forest` and `membership` from the `constructClusters` output), will pick a random edge from the cluster indexed by `clust` and delete it. By deleting this edge, it splits the tree and creates an additional cluster and we say that a cluster is 'born'.

Note that `clust` is an optional parameter; cluster 1 was chosen for demonstration since in this example it happened to be the largest cluster. In practice `clust` is generally not specified, in which case a cluster is chosen at random from the existing clusters with at least one edge.


```{r cluster-birth, fig.width = 6, fig.height = 6}
birth_output = graphBirth(clustered_output$spanning_forest, clustered_output$membership, clust = 1)
ggraph(birth_output$graph, layout = data_hole) +
  geom_edge_link0(edge_alpha = 0.1) +
  geom_node_point(aes(colour = as.factor(birth_output$membership))) +
  scale_colour_brewer(palette = "Paired") + 
  labs(colour = "Cluster")
```

As we can see from the plot above, when compared with the original spanning forest graph, the original cluster 1 was split into the above clusters 1 and 3.

## Death Move

Next, we will consider the opposite of a cluster birth move: a cluster death move. The `graphDeath` function, when provided with a clustered spanning forest graph, corresponding membership, and the *original full graph* (`hole_graph` in this example), will randomly select an edge from the original graph which connects two clusters and add it back to the graph. In doing so, the two clusters that are connected become a single cluster and we say that a cluster has 'died'.

```{r cluster-death, fig.width = 6, fig.height = 6}
death_output = graphDeath(birth_output$graph, birth_output$membership, hole_graph)
ggraph(death_output$graph, layout = data_hole) +
  geom_edge_link0(edge_alpha = 0.1) +
  geom_node_point(aes(colour = as.factor(death_output$membership))) +
  scale_colour_brewer(palette = "Paired") + 
  labs(colour = "Cluster")
```

As we can see from the plot above, when compared with the plot from the birth move, the cluster which was born was subsequently killed (such is life for a cluster) and we are back to the original spanning forest we started with.

## Change Move

A change cluster move is simply the combination of a death move followed by a birth move. The `graphChange` function, when provided with a clustered spanning forest graph, corresponding membership, and the original full graph, will kill a random cluster then birth a new one. The motivation for this is to encourage faster cluster movement and discourage the graph from settling into a local 'optimum' which a single death or birth move alone would be unlikely to remove it from.

```{r cluster-change, fig.width = 6, fig.height = 6}
change_output = graphChange(death_output$graph, death_output$membership, hole_graph)
ggraph(change_output$graph, layout = data_hole) +
  geom_edge_link0(edge_alpha = 0.1) +
  geom_node_point(aes(colour = as.factor(change_output$membership))) +
  scale_colour_brewer(palette = "Paired") + 
  labs(colour = "Cluster")
```

As we can see from the plot above, when compared with the plot from the death move, the previous cluster 5 is killed by merging with cluster 2, then cluster 3 birthed a new cluster 10.

## Hyper Move

The final core move is a hyper cluster move, performed by the `graphHyper` function. In a hyper cluster move, every edge from the original graph that connects vertices belonging to the same cluster is randomly assigned a new weight from a $\textrm{Unif}(0,0.5)$ distribution. Every edge from the original graph that connects vertices belonging to different clusters is randomly assigned a new weight from a $\textrm{Unif}(0.5,1)$ distribution. Then, a new minimum spanning tree across the entire original graph is created using Prim's algorithm. Finally, all the edges which connect different clusters are removed.

The end result is a graph that has the same cluster membership for each vertex, but a different set of active edges comprising each tree in the spanning forest. This is beneficial because it changes the available edges that could be deleted in a birth move.

```{r cluster-hyper, fig.width = 6, fig.height = 6}
hyper_output = graphHyper(hole_graph, change_output$membership)
ggraph(hyper_output$graph, layout = data_hole) +
  geom_edge_link0(edge_alpha = 0.1) +
  geom_node_point(aes(colour = as.factor(hyper_output$membership))) +
  scale_colour_brewer(palette = "Paired") + 
  labs(colour = "Cluster")
```

As we can see from the plot above, when compared with the plot from the change move, every vertex belongs to the same cluster (the membership is unchanged), but the set of edges connecting the vertices within each cluster is different.

# Simulation Study

We now have all the necessary building blocks to formulate a method for predicting the value of our response at new data points. We will use the response data that our spaghettified space surveyors gathered to train our model, then use that model for prediction.

So, let's put it all together in a very simple estimation procedure. Suppose we want to try and approximate the response as a piecewise constant function over the domain. Our estimate for the piecewise constant of each cluster will be the mean of the response at the cluster's member vertices, and the per-cluster error will be the sum of squared difference between each response and our estimate. We'll assess the total error of our graph by summing the error of each cluster, and penalize the number of clusters by adding a scaled exponential function of the number of clusters to the total error.

Note that the choices we are making here are practically arbitrary; there are lots of valid choices for what loss and penalty functions to use as well as different ways to approximate the response using the clusters. Different choices will lead to different performance characteristics and theoretical guarantees; these choices were made because they are simple to demonstrate. The performance and theory of these different choices is an area of ongoing research.

```{r estimation-functions}
estimation_procedure = function(membership, response) {
  k = length(unique(membership))
  estimates = rep(0, k)
  errors = rep(0, k)
  point_estimates = rep(0, length(membership))
  for(i in 1:k) {
    estimates[i] = mean(response[membership == i])
    errors[i] = sum((response[membership == i] - estimates[i])^2)
    point_estimates[membership == i] = estimates[i]
  }
  total_penalty = sum(errors) + (exp(k)/length(membership))
  return(list(total_penalty = total_penalty, 
              errors = errors, 
              estimates = estimates, 
              point_estimates = point_estimates))
}
```

Now, we'll repeatedly execute the 4 different graph operations on our original ```hole_graph``` with different probabilities. We can again arbitrarily choose these probabilities; let's choose a 2/5 chance for a birth move, 2/5 for a death move, and 1/10 each for a hyper move or change move. In every iteration, we will select a move with the probabilities mentioned, evaluate if the error increases or decreases, and accept the new graph if its error decreased. 

Note that since edges play no role in our procedure, a hyper move will always be accepted as it does not affect the total penalty. As above, this way of determining whether to accept a new graph is neither the only way nor necessarily the best; it is chosen for simplicity.

```{r big-loop}
iterations = 2000
current_graph = clustered_output$spanning_forest
current_membership = clustered_output$membership
current_loss = estimation_procedure(current_membership, response)$total_penalty
for(i in 1:iterations) {
  move = sample(1:4, 1, prob = c(0.4, 0.4, 0.1, 0.1))
  if(move == 1) {
    proposed_output = graphBirth(current_graph, current_membership)
    proposed_estimation = estimation_procedure(proposed_output$membership, response)
    if(proposed_estimation$total_penalty <= current_loss) {
      current_graph = proposed_output$graph
      current_membership = proposed_output$membership
      current_loss = proposed_estimation$total_penalty
    }
  } else if(move == 2) {
    proposed_output = graphDeath(current_graph, current_membership, hole_graph)
    proposed_estimation = estimation_procedure(proposed_output$membership, response)
    if(proposed_estimation$total_penalty <= current_loss) {
      current_graph = proposed_output$graph
      current_membership = proposed_output$membership
      current_loss = proposed_estimation$total_penalty
    }
  } else if(move == 3) {
    proposed_output = graphChange(current_graph, current_membership, hole_graph)
    proposed_estimation = estimation_procedure(proposed_output$membership, response)
    if(proposed_estimation$total_penalty <= current_loss) {
      current_graph = proposed_output$graph
      current_membership = proposed_output$membership
      current_loss = proposed_estimation$total_penalty
    }
  } else if(move == 4) {
    proposed_output = graphHyper(hole_graph, current_membership)
    current_graph = proposed_output$graph
    current_membership = proposed_output$membership
  }
}
final_graph = current_graph
final_membership = current_membership
final_estimation_output = estimation_procedure(current_membership, response)
final_penalty = final_estimation_output$total_penalty
final_errors = final_estimation_output$errors
```

Let's see how our estimation did. As a reminder, here is our original graph:

```{r original-graph, fig.width = 6, fig.height = 6}
ggraph(hole_graph, layout = data_hole) + 
  geom_edge_link0(edge_alpha = 0.1) + 
  geom_node_point(aes(color = response)) +
  scale_color_viridis() +
  labs(color = "Response")
```

And here is our model, the piecewise constant approximation:

```{r estimate-graph, fig.width = 7, fig.height = 6}
final_point_estimates = final_estimation_output$point_estimates
ggraph(final_graph, layout = data_hole) + 
  geom_edge_link0(edge_alpha = 0.1) + 
  geom_node_point(aes(color = final_point_estimates)) +
  scale_color_viridis() +
  labs(color = "Response Estimate")
```



## Predicting on Test Data

Now that we have a model, let's predict at all the points our space surveyors missed. Our test data points are stored in `test_data_hole`. The method by which we will predict is by taking each point to predict at, finding the closest vertex in our model (by Euclidean distance), and assigning it to the same cluster. Below is some code to find the closest vertex for each new point:

```{r prediction-data, echo = FALSE}
test_x = runif(500, min = -10, max = 10)
test_y = runif(500, min = -10, max = 10)
test_data_full = data.frame(x = test_x, y =  test_y)
test_data_hole = test_data_full[test_x^2 + test_y^2 >= 16, ]
test_response = 1.2^(test_data_hole$x) + 
  (test_data_hole$y)^2 - 
  5*(test_data_hole$x > test_data_hole$y) + 
  4*test_data_hole$x*test_data_hole$y
```

```{r prediction-method}
closest_point = function(new_point, existing_points) {
  return(unname(which.min(sqrt(colSums((new_point - t(existing_points))^2)))))
}

test_membership = rep(0, nrow(test_data_hole))
for(i in 1:nrow(test_data_hole)) {
  test_membership[i] = final_membership[closest_point(as.numeric(test_data_hole[i,]), data_hole)]
}

test_estimation_output = estimation_procedure(test_membership, test_response)
test_pred_response = test_estimation_output$point_estimates
```

Here is what the true response function applied to our testing data looks like:

```{r true-test-graph, fig.width = 7, fig.height = 6}
ggplot(test_data_hole) + 
  geom_point(aes(x, y, color = test_response)) +
  scale_color_viridis(limits = c(-300, 500)) +
  labs(color = "Response")
```

And here is what our model predicted the response function of our testing data to look like:

```{r pred-test-graph, fig.width = 7, fig.height = 6}
ggplot(test_data_hole) + 
  geom_point(aes(x, y, color = test_pred_response)) +
  scale_color_viridis(limits = c(-300, 500)) +
  labs(color = "Predicted Response")
```

Not bad for such a simple procedure! Here is a histogram of the prediction errors from this method:

```{r residual, fig.width = 7, fig.height = 6}
test_error = test_response - test_pred_response
ggplot() + 
  geom_histogram(aes(x = test_error), bins = 20) +
  labs(x = "Prediction Error")
```

And a graph of where these errors occur (dotted line overlays denote where discontinuities in the true response function lie):

```{r residual-graph, fig.width = 7, fig.height = 6}
ggplot(test_data_hole) + 
  geom_point(aes(x, y, color = abs(test_error))) +
  geom_abline(intercept = 0, slope = 1, linetype = "dotted") +
  geom_abline(intercept = 0, slope = -1, linetype = "dotted") +
  scale_color_viridis() +
  labs(color = "Absolute Prediction Error")
```


As we can see, even with a very simple combination of the four core graph operations we can recover a lot of information about the true response function.




However, with a more detailed model using *multiple* weak learners, we can do even better. Let's see how effective the BAST model is at predicting in this situation. First, we'll choose our hyperparameters. For more details, see the original paper [Luo, Z. T., Sang, H., & Mallick, B. (2021) BAST: Bayesian Additive Regression Spanning Trees for Complex Constrained Domain. Advances in Neural Information Processing Systems 34 (NeurIPS 2021)](https://proceedings.neurips.cc/paper/2021/hash/00b76fddeaaa7d8c2c43d504b2babd8a-Abstract.html)

The most important choices for selecting hyperparameters is choosing the number of weak learners, and what the maximum number of clusters each weak learner can contain is. Too many weak learners is computationally expensive, too few will result in suboptimal predictive performance. 

```{r standardize-funcs, include=FALSE}
# function to standardize Y
standardize <- function(x) {
  xmean = mean(x)
  x = x - xmean
  xscale = 2 * max(abs(x))
  x = x / xscale
  param = c('mean' = xmean, 'scale' = xscale)
  return(list(x = x, std_par = param))
}

# function to unstandardize Y
unstandardize <- function(x, std_par, nomean = F, s2 = F) {
  if(s2) {
    x = x * std_par['scale'] ^ 2
  } else {
    x = x * std_par['scale']
  }
  if(!nomean) x = x + std_par['mean']
  return(x)
}
```

```{r hyperpars}
M = 30      # number of weak learners
k_max = 6   # maximum number of clusters per weak learner
mu = list() # initial values of mu (piecewise constant value to fit to cluster)
n = length(response)
cluster = matrix(1, nrow = n, ncol = M)  # initial cluster memberships
for(m in 1:M) {
  mu[[m]] = c(0)
}

init_val = list()
init_val[['mu']] = mu
init_val[['sigmasq_y']] = 1

# standardize our response
std_res = standardize(response)
Y_std = std_res$x
std_par = std_res$std_par

# find lambda_s
nu = 3 
q = 0.9
quant = qchisq(1-q, nu)
lambda_s = quant * var(Y_std) / nu

hyperpar = c()
hyperpar['sigmasq_mu'] = (0.5/(2*sqrt(M)))^2
hyperpar['lambda_s'] = lambda_s
hyperpar['nu'] = nu
hyperpar['lambda_k'] = 4
hyperpar['M'] = M
hyperpar['k_max'] = k_max

# MCMC parameters
# number of posterior samples = (MCMC - BURNIN) / THIN
MCMC = 1000    # MCMC iterations
BURNIN = 500  # burnin period length
THIN = 5        # thinning intervals

```

Note that we have chosen to only perform 1000 iterations compared to 2000 iterations in the previous setup, and will only get 100 posterior samples.

Now, we'll fit the model:

```{r bast-fit, cache=TRUE}
BAST_model = BASTIONfit_C(Y_std, hole_graph, init_val, hyperpar, MCMC, BURNIN, THIN, seed = 1234)
```

```{r pred-funcs, include=FALSE}
predictBAST <- function(mcmc_res, coords, coords_new, method = 'soft-knn', 
                        mesh = NULL, return_type = 'mean', weighting = 'uniform', 
                        cdist_mat = NULL, k_nn = 5, seed = 12345) {
  set.seed(seed)
  
  # get posterior mean in-sample prediction
  npost = length(mcmc_res$log_post_out)
  cluster_out = mcmc_res$cluster_out
  mu_out = mcmc_res$mu_out
  
  M = dim(cluster_out)[3]
  n = dim(cluster_out)[2]
  
  mu_all = array(0, dim = c(npost, n, M))
  for(i in 1:npost) {
    cluster_i = cluster_out[i, , , drop = F]
    mu_i = mu_out[[i]]
    mu_all_i = matrix(0, ncol = M, nrow = n) # n*M matrix
    for(m in 1:M) {
      mu_all_i[, m] = mu_i[[m]][cluster_i[1, , m]]
    }
    mu_all[i, , ] = mu_all_i
  }
  Y_hat_all = t(apply(mu_all, c(1, 2), sum))
  
  if(method == 'soft-mesh' | method == 'soft-knn') {
    ## soft prediction
    ## method == 'soft-mesh': soft prediction using FEM mesh (for 2-d domains only)
    ## method == 'soft-knn': soft prediction based on KNN graph
    
    Y_new_hat = softPredict2(mu_all, coords, coords_new, method, mesh,
                             return_type, weighting, cdist_mat, k_nn)
  } else {
    stop('Unsupported method')
  }
  
  return(Y_new_hat)
}

# function to get adjacency list for boundary mesh nodes
getBndAdjList <- function(mesh) {
  n_int = mesh$n_int  # number of interior nodes
  n_bnd = nrow(mesh$nodes) - n_int  # number of boundary nodes
  edge_list = mesh$edges[mesh$bnd_edges, ]
  
  adj_list = rep(list(c()), n_bnd)  # empty list
  names(adj_list) = as.character((n_int + 1):(n_int + n_bnd))
  for(i in 1:nrow(edge_list)) {
    endpoints = edge_list[i, ]
    v_bnd = max(endpoints); v_int = min(endpoints)
    if(v_int > n_int) next
    v_bnd_char = as.character(v_bnd)
    if(is.null(adj_list[[v_bnd_char]])) {
      adj_list[[v_bnd_char]] = c(v_int)
    } else {
      adj_list[[v_bnd_char]] = c(adj_list[[v_bnd_char]], v_int)
    }
  }
  return(adj_list)
}

# function to get neighbor probability matrix for new locations given a mesh
# nn_prob[i, j]: probability that node j is used for prediction at new location i
# ncol(nn_prob) == number of nodes in mesh (including boundary nodes)
getNNProb <- function(mesh, coords_new, type = 'uniform', b = 1) {
  if(type == 'uniform' | type == 'distance') {
    tri_idx = apply(coords_new, 1, function(loc) R_insideIndex2(mesh, loc, all_tri = T))
    nn_prob = matrix(0, nrow = nrow(coords_new), ncol = nrow(mesh$nodes))
    
    n_nodes = nrow(mesh$nodes); n_int = mesh$n_int
    if(type == 'distance') {
      weights = 1 / as.matrix(rdist(coords_new, mesh$nodes)) ^ b
    } else {
      weights = matrix(1, nrow = nrow(coords_new), ncol = nrow(mesh$nodes))
    }
    
    if(class(tri_idx) == 'list') {
      for(i in 1:nrow(coords_new)) {
        nn_idx = mesh$triangles[tri_idx[[i]], ]
        nn_idx = unique(c(nn_idx))
        # use boundary nodes only if all neighbors are on boundary
        if(!all(nn_idx > n_int))
          nn_idx = nn_idx[nn_idx <= n_int]
        nn_prob[i, nn_idx] = weights[i, nn_idx]
      }
      
    } else { # tri_idx is a vector
      for(i in 1:nrow(coords_new)) {
        nn_idx = mesh$triangles[tri_idx[i], ]
        # use boundary nodes only if all neighbors are on boundary
        if(!all(nn_idx > n_int))
          nn_idx = nn_idx[nn_idx <= n_int]
        nn_prob[i, nn_idx] = weights[i, nn_idx]
      }
    }
    
    # normalize nn_prob
    nn_prob = nn_prob / rowSums(nn_prob)
    
  }  else {
    stop('Unsupported weighting types')
  }
  
  return(nn_prob)
}

# extend prediction to boundary nodes of a mesh
getBndPred <- function(Y_hat, mesh, adj_list_bnd = NULL) {
  if(is.null(adj_list_bnd))
    adj_list_bnd = getBndAdjList(mesh)
  n = length(Y_hat)  # number of interior nodes
  n_bnd = length(adj_list_bnd)  # number of boundary nodes
  Y_hat_bnd = rep(0, n_bnd)
  for(i in (n + 1):(n + n_bnd))
    Y_hat_bnd[i - n] = mean(Y_hat[ adj_list_bnd[[as.character(i)]] ])
  return(Y_hat_bnd)
}


# helper function for soft prediction
softPredict2 <- function(mu_all, coords, coords_new, method = 'soft-knn', 
                         mesh = NULL, return_type = 'mean', 
                         weighting = 'uniform', cdist_mat = NULL, k_nn = 5) {
  extend_bnd = F  # do we have to extend prediction to boundary nodes in mesh
  if(method == 'soft-mesh') {
    # soft prediction using mesh
    # require(fdaPDE)
    if(is.null(mesh)) 
      stop("Missing 'mesh'.")
    
    # get neighbor choices for new locations
    nn_prob = getNNProb(mesh, coords_new, weighting)
    nn_prob = round(nn_prob, 8)
    
    # check if boundary nodes are useful for prediction
    n_int = mesh$n_int
    if(any(nn_prob[, (n_int+1):ncol(nn_prob)] > 0))
      extend_bnd = T
    
    # pre-compute adjacency list for boundary mesh nodes
    if(extend_bnd)
      adj_list_bnd = getBndAdjList(mesh)
    
  } else if(method == 'soft-knn') {
    # soft prediction using constrained KNN
    require(fields)
    
    if(is.null(cdist_mat))
      cdist_mat = as.matrix(rdist(coords_new, coords))
    nn_list = KNNGraph(cdist_mat, k_nn, cross_dist = T, return_graph = F)

    # get probability for each neighbor
    n_new = nrow(coords_new)
    nn_prob = matrix(0, nrow = n_new, ncol = nrow(coords))
    if(weighting == 'uniform') {
      for(i in 1:n_new)
        nn_prob[i, nn_list[[i]] ] = 1
    } else if(weighting == 'distance') {
      # deal with zero distance
      cdist_mat[cdist_mat == 0] = 1e-9
      for(i in 1:n_new)
        nn_prob[i, nn_list[[i]] ] = 1 / cdist_mat[i, nn_list[[i]] ]
    } else {
      stop('Unsupported weighting types')
    }
    # normalize nn_prob
    nn_prob = nn_prob / rowSums(nn_prob)
  }
  
  n_nodes = ncol(nn_prob)
  npost = dim(mu_all)[1]
  M = dim(mu_all)[3]
  Y_new_all = matrix(0, nrow = nrow(coords_new), ncol = npost)
  
  
  for(i in 1:npost) {
    # sample neighbors
    nn = t(apply(nn_prob, 1, function(p) sample.int(n_nodes, M, replace = T, prob = p)))
    
    mu_i = mu_all[i, , ]
    
    if(method == 'soft-mesh' & extend_bnd) {
      # extend mu to boundary nodes in FEM mesh
      mu_i_bnd = apply(mu_i, 2, getBndPred, mesh = mesh, adj_list_bnd = adj_list_bnd)
      mu_i = rbind(mu_i, mu_i_bnd)
    }
    
    mu_new_i = matrix(0, nrow = nrow(coords_new), ncol = M)
    for(m in 1:M)
      mu_new_i[, m] = mu_i[nn[, m], m]
    Y_new_all[, i] = rowSums(mu_new_i)
  }
  if(return_type == 'mean') {
    Y_new_hat = rowMeans(Y_new_all)
  } else {
    Y_new_hat = Y_new_all
  }
  return(Y_new_hat)
}

gen2dMesh <- function(coords, bnd, ...) {
  # note the first and last boundary nodes are the same
  n = nrow(coords); n_bnd = length(bnd$x) - 1
  coords_all = rbind(coords, cbind(bnd$x, bnd$y)[1:n_bnd, ])
  
  # get boundary segments
  segments = cbind( (n+1):(n+n_bnd), c((n+2):(n+n_bnd), n+1) )
  
  mesh = create.mesh.2D(coords_all, segments = segments, ...)
  mesh$n_int = n  # number of interior nodes
  # edges that connect boundary nodes
  mesh$bnd_edges = apply(mesh$edges, 1, FUN = function(x) any(x > n))
  return(mesh)
}

R_insideIndex2 <- function (mesh0, location, all_tri = FALSE) {
  eps = 2.2204e-016
  small = 10000 * eps
  
  nodes = mesh0$nodes
  triangles = mesh0$triangles
  X = location[1]
  Y = location[2]
  
  ntri = dim(triangles)[[1]]
  indtri = matrix(1:ntri, ncol = 1)
  
  # compute coefficients for computing barycentric coordinates if needed
  
  tricoef = R_tricoefCal(mesh0)
  
  # compute barycentric coordinates
  r3 = X - nodes[triangles[, 3], 1]
  s3 = Y - nodes[triangles[, 3], 2]
  lam1 = (tricoef[, 4] * r3 - tricoef[, 2] * s3)
  lam2 = (-tricoef[, 3] * r3 + tricoef[, 1] * s3)
  lam3 = 1 - lam1 - lam2
  
  # test these coordinates for a triple that are all between 0 and 1
  int  = (-small <= lam1 & lam1 <= 1 + small) &
    (-small <= lam2 & lam2 <= 1 + small) &
    (-small <= lam3 & lam3 <= 1 + small)
  
  # return the index of this triple, or NaN if it doesn't exist
  indi = indtri[int]
  if (length(indi) < 1)
  {
    ind = NA
  } else if (all_tri) {
    ind = indi
  } else{
    ind = min(indi)
  }
  return(ind)
}

R_tricoefCal = function(mesh)
{
  #  TRICOEFCAL compute the coefficient matrix TRICOEF
  #  required to test of a point is indside a triangle
  
  nodes = mesh$nodes
  triangles = mesh$triangles
  
  ntri   = dim(triangles)[[1]]
  
  #  compute coefficients for computing barycentric coordinates if
  #  needed
  
  tricoef = matrix(0, nrow=ntri, ncol=4)
  tricoef[,1] = nodes[triangles[,1],1]-nodes[triangles[,3],1]
  tricoef[,2] = nodes[triangles[,2],1]-nodes[triangles[,3],1]
  tricoef[,3] = nodes[triangles[,1],2]-nodes[triangles[,3],2]
  tricoef[,4] = nodes[triangles[,2],2]-nodes[triangles[,3],2]
  detT = matrix((tricoef[,1]*tricoef[,4] - tricoef[,2]*tricoef[,3]),ncol=1)
  tricoef = tricoef/(detT %*% matrix(1,nrow=1,ncol=4))
  
  return(tricoef)
}
```

<!-- Let's look at the prediction field generated by the BAST model: -->



```{r bound, cache=TRUE, include=FALSE}
bnd_inner = t(sapply(1:50,function(r, q){c(q*cos(2*r*pi/50),q*sin(2*r*pi/50))}, q = 4))
bnd_outer = list(x = c(rep(-10, 30), seq(-10, 10, length.out = 30), rep(10, 30), seq(10, -10, length.out = 30)), y = c(seq(-10, 10, length.out = 30), rep(10, 30), seq(10, -10, length.out = 30), rep(-10, 30)))
bnd = list(x = c(bnd_outer$x, bnd_inner[,1]), y = c(bnd_outer$y, bnd_inner[,2]))
bnd_mat = matrix(c(bnd$x, bnd$y), nrow = 170)
bnd_mat = bnd_mat[!duplicated(bnd_mat), ]
bnd_out = list(x = bnd_mat[1:116,1], y = bnd_mat[1:116,2])
bnd_in = list(x = bnd_mat[117:166,1], y = bnd_mat[117:166,2])

lon_unique = sort(unique(data_hole[, 1]))
lat_unique = sort(unique(data_hole[, 2]))
lon_gap = lon_unique[2] - lon_unique[1]
lat_gap = lat_unique[2] - lat_unique[1]
lon_min = lon_unique[1]; lon_max = lon_unique[length(lon_unique)]
lat_min = lat_unique[1]; lat_max = lat_unique[length(lat_unique)]

data_hole_grid = expand.grid(seq(lon_min - lon_gap, lon_max + lon_gap, lon_gap / 2),
                          seq(lat_min - lat_gap, lat_max + lat_gap, lat_gap / 2))
data_hole_grid = as.matrix(data_hole_grid)
inside_bnd = apply(data_hole_grid, 1, function(coord) {
  x = coord[1]; y = coord[2]
  mgcv::inSide(bnd, x, y)
})
data_hole_grid = data_hole_grid[inside_bnd, ]
```

```{r gen-mesh, include = FALSE}
mesh = gen2dMesh(as.matrix(data_hole), bnd_out, holes = bnd_mat[117:166, ])
```


<!-- ```{r prediction-bast, cache=TRUE, include=FALSE} -->
<!-- prediction_scheme = function(fitted_model) { -->
<!--   Y_grid_all = predictBAST(fitted_model,  -->
<!--                            data_hole,  -->
<!--                            data_hole_grid,  -->
<!--                            method = 'soft-mesh',  -->
<!--                            mesh = mesh,  -->
<!--                            weighting = 'uniform',  -->
<!--                            return_type = 'all',  -->
<!--                            seed = 12345) -->
<!--   Y_grid_all = apply(Y_grid_all, 2, unstandardize, std_par = std_par) -->

<!--   # use posterior mean as predictor -->
<!--   Y_grid = rowMeans(Y_grid_all) -->

<!--   # back to original scale -->
<!--   Y_grid = Y_grid + mean(response) -->
<!--   return(Y_grid) -->
<!-- } -->

<!-- Y_grid = prediction_scheme(BAST_model) -->
<!-- ``` -->

<!-- ```{r prediction-field, fig.width = 7, fig.height = 6} -->
<!-- plotField(coords_grid, Y_grid_BASTION_C, title = 'Predictive Field - BASTION (C++)') + -->
<!--   xlab('X') +  -->
<!--   ylab('Y') -->
<!-- ``` -->



